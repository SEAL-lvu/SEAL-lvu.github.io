<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SEAL-lvu</title>
<link href="./style.css" rel="stylesheet">
</head>

<body>
<div class="content">
  <h1><img src="./data/logo.png" alt="Seal Figure" width="50" style="vertical-align: middle;" /><strong>SEAL: Semantic Attention Learning for Long Video Representation</strong></h1>
  <p id="authors"><a href="https://lan-lw.github.io/" style="color:blue;">Lan Wang<sup>1,2</sup></a> <a href="https://issaccyj.github.io/"  style="color:blue;">Yujia Chen<sup>2</sup></a> <a href="https://dutran.github.io/"  style="color:blue;">Du Tran<sup>2</sup></a> <br></a><a href="https://vishnu.boddeti.net/"  style="color:blue;">Vishnu Boddeti<sup>1</sup></a> <a href="https://l2ior.github.io/"  style="color:blue;">Wen-Sheng Chu<sup>2</sup></a><br>

    <span style="font-size: 16px"><br>
        <sup>1</sup> Michigan State University  &nbsp  <sup>2</sup> Google <br>     
        </p>

  <img src="./data/web_teaser.png" class="teaser-gif" style="width:100%;">
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/pdf/2412.01798" target="_blank">[arXiv]</a> &nbsp;&nbsp;&nbsp;&nbsp;
<!-- 	          <a href="https://github.com/SEAL-lvu/SEAL-lvu.github.io/" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp; -->
          </p>
    </font>
    <p>
  <strong>Long Video Representation with Semantic Attention Learning (SEAL):</strong> 
  Conventional uniform sampling results in redundant and cluttered visual information, making it difficult for both AI models and human brains to process efficiently. 
  Decomposing long videos into semantic entities such as scenes, objects, and actions reduces temporal redundancy, thus making model training and inference more efficient. 
  In this example, the long video <strong>ùí±</strong> is decomposed into 4 <strong>scene tokens</strong> (S1--S4), 6 <strong>object tokens</strong> (O1--O6), and 4 <strong>action/event tokens</strong> (A1--A4). 
  A query-aware attention learning module improves downstream task performance by focusing on relevant information rather than processing everything, as shown with queries (Q1--Q4) and their most relevant tokens. 
</p>
</div>


<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Long video understanding presents challenges due to the inherent high computational complexity and redundant temporal information. An effective representation for long videos must process such redundancy efficiently while preserving essential contents for downstream tasks. This paper introduces SEmantic Attention Learning (SEAL), a novel unified representation for long videos. To reduce computational complexity, long videos are decomposed into three distinct types of semantic entities: scenes, objects, and actions, allowing models to operate on a handful of entities rather than a large number of frames or pixels. To further address redundancy, we propose an attention learning module that balances token relevance with diversity formulated as a subset selection optimization problem. Our representation is versatile, enabling applications across various long video understanding tasks. Extensive experiments show that SEAL significantly outperforms state-of-the-art methods in video question answering and temporal grounding tasks and benchmarks including LVBench, MovieChat-1K, and Ego4D.</p>
</div>

<div class="content">
    <h2>Contributions</h2>
    <ul>
      <li>We introduce SEAL, a novel unified representation for long videos by decomposing them into semantic tokens, namely scenes, objects, and actions.</li>
      <p></p>
      <li>Our attention learning module reduces temporal redundancy while supporting strong cross-task generalization.
    We show SEAL works in both global and streaming modes, making it adaptable to arbitrarily long videos.
      </li>
      <p></p>
      <li>Significantly outperforms state-of-the-art methods on various long video understanding tasks and benchmarks including: video QA (MovieChat-1K, LVBench), and egocentric video grounding (Ego4D).</li>
    </ul>        
    </div>

<div class="content">        
    <h2>Method</h2>
    <img class="summary-img" src="./data/web_lvu.png" style="width:100%;"> <br>
    <p>
  During <em>semantic decomposition</em>, a long video <strong>ùí±</strong> is decomposed into semantic tokens representing scenes, objects, and actions. 
  Then, during <em>attention learning</em>, these tokens and the query <strong>q</strong> are optimized for query relevance <strong>R(‚ãÖ)</strong> and token diversity <strong>S(‚ãÖ)</strong>. 
  The resulting attended token subset is then passed to a vision or an MLLM head for predictions.
</p>   
  </div>

<div class="content">        
    <h2>Qualitative Results</h2>
    <img class="summary-img" src="./data/web_vis.png" style="width:100%;"> <br>
    <p> Two long videos visualized with questions, multiple choice options, and SEAL predicted answers. SEAL attends to relevant entities such as ''royal family'' and ''stool'' (Q1.a), different ''meals'' and ''drinks'' (Q2.a), ''scene'' and ''location'' (Q2.b) and correctly answers these questions. Although attending to relevant ''push-up'' activity (Q2.c), SEAL fails to predict the right answer due to the challenging in the causal reasoning question.</p>    
  </div>


<div class="content">
  <h2>BibTex</h2>
  <code> @article{lan2024seal,<br>
  &nbsp;&nbsp;title={SEAL: Semantic Attention Learning for Long Video Representation},<br>
  &nbsp;&nbsp;author={Wang, L and Chen, Y and Tran, D and Boddeti, V and Chu, W},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2412.01798},<br>
  &nbsp;&nbsp;year={2024}<br>
  } </code> 
</div>


</body>


</html>
