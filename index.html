<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>RB-Modulation</title>
<link href="./style.css" rel="stylesheet">
<!-- <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script> -->
</head>

<body>
<div class="content">
  <h1><img src="./data/logo.png" alt="Seal Figure" width="50" style="vertical-align: middle;" /><strong>SEAL: Semantic Attention Learning for Long Video Representation</strong></h1>
  <!-- <h2 style="text-align:center;">CVPR 2024</h2> -->
  <p id="authors"><a href="https://lan-lw.github.io/" style="color:blue;">Lan Wang<sup>1,2</sup></a> <a href="https://issaccyj.github.io/"  style="color:blue;">Yujia Chen<sup>2</sup></a> <a href="https://l2ior.github.io/"  style="color:blue;">Wen-Sheng Chu<sup>2</sup></a> <br></a><a href="https://vishnu.boddeti.net/"  style="color:blue;">Vishnu Boddeti<sup>1</sup></a> <a href="https://dutran.github.io/"  style="color:blue;">Du Tran<sup>2</sup></a><br>

    <span style="font-size: 16px"><br>
        <sup>1</sup> Michigan State University  &nbsp  <sup>2</sup> Google <br>     
        </p>

  <img src="./data/web_teaser.png" class="teaser-gif" style="width:100%;">
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/xxxx" target="_blank">[arXiv]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	          <a href="https://github.com/lan-lw/SEAL.github.io/" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
          </p>
    </font>
    <p>**Long Video Representation with Semantic Attention Learning (SEAL):** Conventional uniform sampling results in redundant and cluttered visual information, making it difficult for both AI models and human brains to process efficiently. Decomposing long videos into semantic entities such as scenes, objects, and actions reduces temporal redundancy, thus making model training and inference more efficient. In this example, the long video **$\mathcal{V}$** is decomposed into 4 **scene tokens** (S1--S4), 6 **object tokens** (O1--O6), and 4 **action/event tokens** (A1--A4). A query-aware attention learning module improves downstream task performance by focusing on relevant information rather than processing everything, as shown with queries (Q1--Q4) and their most relevant tokens. *(Best viewed in color)*
</p>
</div>


<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Long video understanding presents challenges due to the inherent high computational complexity and redundant temporal information. An effective representation for long videos must process such redundancy efficiently while preserving essential contents for downstream tasks. This paper introduces SEmantic Attention Learning (SEAL), a novel unified representation for long videos. To reduce computational complexity, long videos are decomposed into three distinct types of semantic entities: scenes, objects, and actions, allowing models to operate on a handful of entities rather than a large number of frames or pixels. To further address redundancy, we propose an attention learning module that balances token relevance with diversity formulated as a subset selection optimization problem. Our representation is versatile, enabling applications across various long video understanding tasks. Extensive experiments show that SEAL significantly outperforms state-of-the-art methods in video question answering and temporal grounding tasks and benchmarks including LVBench, MovieChat-1K, and Ego4D.</p>
</div>

<!-- 
<div class="content">
    <h2>Problem Statement</h2>
    <p>In <strong>training-free stylization</strong>, recent methods [12, 13, 14] manipulate keys and values within the attention layers using just one reference style image. These methods face challenges in both extracting the style from the reference style image and accurately transferring the style to a target content image. For instance, during the DDIM inversion step [20] utilized by StyleAligned [12], fine-grained details tend to be compromised. To mitigate this issue, InstantStyle [13] incorporates features from the reference style image into specific layers of a previously trained IP-Adapter [21]. However, identifying the exact layer for feature injection in a model is complex and not universally applicable across models. Also, feature injection can cause content leakage from the style image into the generated content. Moving on to <strong>content-style composition</strong>, InstantStyle [13] employs a ControlNet [22] (an additionally trained network) to preserve image layout, which inadvertently limits its diversity.
    <br> 
    We introduce Reference-Based Modulation (<strong>RB-Modulation</strong>), a novel approach for content and style personalization that eliminates the need for training or finetuning diffusion models (e.g. ControlNet [22] or adapters [21, 9]). By incorporating style features into the controller’s terminal cost, we modulate the drift field in diffusion models’ reverse dynamics, enabling training-free personalization. Further, unlike conventional attention processors that often leak content from the reference style image, we propose an Attention Feature Aggregation (AFA) module that decouples content from the reference style image.
    </p>    
    </div> -->
<div class="content">
    <h2>Contributions</h2>
    <ul>
      <li>We introduce SEAL, a novel unified representation for long videos by decomposing them into semantic tokens, namely scenes, objects, and actions.</li>
      <p></p>
      <li>Our attention learning module reduces temporal redundancy while supporting strong cross-task generalization.
    We show SEAL works in both global and streaming modes, making it adaptable to arbitrarily long videos.
      </li>
      <p></p>
      <li>significantly outperforms state-of-the-art methods on various long video understanding tasks and benchmarks including: video QA (MovieChat-1K, LVBench), and egocentric video grounding (Ego4D).</li>
    </ul>        
    </div>

<div class="content">        
    <h2>Method</h2>
    <img class="summary-img" src="./data/web_lvu.png" style="width:100%;"> <br>
    <p> During *semantic decomposition*, a long video **$\mathcal{V}$** is decomposed into semantic tokens representing scenes, objects, and actions. Then, during *attention learning*, these tokens and the query **q** are optimized for query relevance **R(⋅)** and token diversity **S(⋅)**. The resulting attended token subset is then passed to a vision or an MLLM head for predictions.
</p>    
  </div>

<div class="content">        
    <h2>Qualitative Results</h2>
    <img class="summary-img" src="./data/web_vis.png" style="width:100%;"> <br>
    <p> Two long videos visualized with questions, multiple choice options, and SEAL predicted answers. SEAL attends to relevant entities such as ``royal family'' and ``stool'' (Q1.a), different ``meals'' and ``drinks'' (Q2.a), ``scene'' and ``location'' (Q2.b) and correctly answers these questions. Although attending to relevant ``push-up'' activity (Q2.c), SEAL fails to predict the right answer due to the challenging in the causal reasoning question.</p>    
  </div>


<div class="content">
  <h2>BibTex</h2>
  <code> @article{lan2024seal,<br>
  &nbsp;&nbsp;title={SEAL: Semantic Attention Learning for Long Video Representation},<br>
  &nbsp;&nbsp;author={Wang, L and Chen, Y and Chu, W and Boddeti, V and Tran, D},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:xxxx.xxxx},<br>
  &nbsp;&nbsp;year={2024}<br>
  } </code> 
</div>


</body>


</html>
